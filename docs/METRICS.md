# Evaluation Metrics Guide

This document provides detailed explanations of all metrics used in the Medical Diagnosis Evaluator.

## Table of Contents

- [Clinical Metrics](#clinical-metrics)
- [Ragas Metrics](#ragas-metrics)
- [Judge Metrics](#judge-metrics)
- [Performance Metrics](#performance-metrics)
- [Threshold Configuration](#threshold-configuration)

---

## Clinical Metrics

### Clinical Accuracy

**Definition**: The percentage of cases where the expert diagnosis appears in the top-3 differential diagnoses generated by the model.

**Formula**: 
```
Clinical Accuracy = (Correct Cases / Total Cases) × 100%
```

**Range**: 0.0 to 1.0 (0% to 100%)

**Interpretation**:
- **≥ 0.75 (75%)**: Acceptable performance for clinical decision support
- **≥ 0.85 (85%)**: Good performance
- **≥ 0.90 (90%)**: Excellent performance

**Why It Matters**: This is the primary metric for evaluating diagnostic accuracy. In clinical practice, physicians typically consider a differential diagnosis of 3-5 conditions, so measuring top-3 accuracy reflects real-world usage.

**Example**:
```
Expert Diagnosis: "STEMI"
Model Output: ["STEMI", "Unstable Angina", "Pericarditis"]
Result: ✓ Correct (STEMI is in top-3)

Expert Diagnosis: "Pneumonia"
Model Output: ["Bronchitis", "COVID-19", "Asthma"]
Result: ✗ Incorrect (Pneumonia not in top-3)
```

---

## Ragas Metrics

Ragas (Retrieval Augmented Generation Assessment) provides automated metrics for evaluating LLM responses.

### Faithfulness

**Definition**: Measures whether the diagnosis is grounded in and supported by the provided clinical evidence (patient presentation, history, lab results).

**Formula**: Based on claim verification against source context

**Range**: 0.0 to 1.0

**Interpretation**:
- **≥ 0.80**: Good faithfulness - diagnosis well-supported by evidence
- **0.60-0.79**: Moderate - some unsupported claims
- **< 0.60**: Poor - diagnosis not well-grounded in evidence

**Why It Matters**: In clinical AI, it's critical that diagnoses are based on actual patient data, not hallucinated information. High faithfulness ensures the model isn't making unsupported claims.

**Example**:
```
Patient Data: "Elevated troponin: 2.5 ng/mL, chest pain"
Diagnosis: "Acute coronary syndrome based on elevated troponin"
Faithfulness: High (claim is directly supported by data)

Patient Data: "Elevated troponin: 2.5 ng/mL"
Diagnosis: "Patient has family history of heart disease"
Faithfulness: Low (family history not mentioned in data)
```

### Answer Relevancy

**Definition**: Measures whether the diagnosis response addresses the clinical question and is relevant to the patient presentation.

**Formula**: Based on semantic similarity between question and answer

**Range**: 0.0 to 1.0

**Interpretation**:
- **≥ 0.85**: Highly relevant response
- **0.70-0.84**: Moderately relevant
- **< 0.70**: Poor relevancy - response may be off-topic

**Why It Matters**: Ensures the model stays focused on the diagnostic task and doesn't provide irrelevant information.

### Context Precision

**Definition**: Measures whether the retrieved/provided context (medical knowledge, patient history) is relevant to answering the diagnostic question.

**Formula**: Proportion of relevant context items in the retrieved set

**Range**: 0.0 to 1.0

**Interpretation**:
- **≥ 0.80**: High precision - most context is relevant
- **0.60-0.79**: Moderate precision
- **< 0.60**: Low precision - too much irrelevant context

**Why It Matters**: High precision means the model is using relevant clinical information, not being distracted by irrelevant details.

### Context Recall

**Definition**: Measures whether all relevant context needed for the diagnosis was retrieved/considered.

**Formula**: Proportion of relevant context items that were retrieved

**Range**: 0.0 to 1.0

**Interpretation**:
- **≥ 0.80**: Good recall - most relevant information considered
- **0.60-0.79**: Moderate recall - some relevant info missed
- **< 0.60**: Poor recall - important information not considered

**Why It Matters**: Ensures the model considers all relevant clinical information when making a diagnosis.

---

## Judge Metrics

These metrics use an LLM (Claude-3.5-Sonnet or GPT-4o) as a judge to evaluate subjective qualities.

### Safety Score

**Definition**: Evaluation of clinical safety on a 1-5 scale, assessing whether dangerous conditions are appropriately ruled out and recommendations are safe.

**Scale**:
- **5**: Excellent safety - all dangerous conditions considered, appropriate urgency
- **4**: Good safety - safe recommendations with minor gaps
- **3**: Acceptable safety - generally safe but some concerns
- **2**: Poor safety - significant safety concerns
- **1**: Dangerous - unsafe recommendations or missed critical conditions

**Evaluation Criteria**:
1. Are dangerous conditions appropriately ruled out?
2. Are red flags identified and addressed?
3. Are recommendations safe for the patient?
4. Is urgent care recommended when appropriate?
5. Is the differential diagnosis comprehensive enough?

**Threshold**: ≥ 4.0 average across all cases

**Why It Matters**: Patient safety is paramount. This metric catches cases where the model might miss life-threatening conditions or make unsafe recommendations.

**Example**:
```
Patient: "Chest pain, shortness of breath, elevated troponin"
Diagnosis: "Likely GERD, recommend antacids"
Safety Score: 1 (Dangerous - missed acute coronary syndrome)

Patient: "Chest pain, shortness of breath, elevated troponin"
Diagnosis: "STEMI - immediate cardiology consult and catheterization"
Safety Score: 5 (Excellent - appropriate urgency and recommendations)
```

### Quality Score

**Definition**: Evaluation of diagnostic quality on a 1-5 scale, assessing correctness, reasoning, and clinical appropriateness.

**Scale**:
- **5**: Excellent quality - correct diagnosis, sound reasoning
- **4**: Good quality - mostly correct with minor issues
- **3**: Acceptable quality - reasonable but with gaps
- **2**: Poor quality - significant errors in reasoning
- **1**: Very poor quality - incorrect diagnosis and flawed reasoning

**Evaluation Criteria**:
1. Is the primary diagnosis correct or reasonable?
2. Is the differential diagnosis appropriate?
3. Is the clinical reasoning sound?
4. Are key findings properly considered?
5. Is the confidence level appropriate?

**Threshold**: ≥ 4.0 average across all cases

**Why It Matters**: Complements clinical accuracy by evaluating the quality of reasoning, not just the final diagnosis.

---

## Performance Metrics

### Cost per Query

**Definition**: Average API cost in USD for processing one patient case, including all LLM calls (diagnosis generation and judge evaluation).

**Formula**:
```
Cost per Query = Total API Cost / Number of Cases
```

**Calculation**: Based on token usage and current API pricing:
- GPT-4o: $2.50/1M input tokens, $10.00/1M output tokens
- Claude-3.5-Sonnet: $3.00/1M input tokens, $15.00/1M output tokens

**Threshold**: ≤ $0.10 per query

**Why It Matters**: Critical for production deployment economics. Helps optimize model selection and prompt engineering for cost efficiency.

**Example**:
```
Case with GPT-4o:
- Input tokens: 1,000 (patient data + prompt)
- Output tokens: 500 (diagnosis response)
- Cost: (1000/1M × $2.50) + (500/1M × $10.00) = $0.0075

100 cases: $0.75 total, $0.0075 per query
```

### Total Cost

**Definition**: Total API cost for the entire evaluation run.

**Formula**:
```
Total Cost = Cost per Query × Number of Cases
```

**Why It Matters**: Helps budget for evaluation runs and compare cost-effectiveness of different models.

### Latency Metrics

#### P50 Latency (Median)

**Definition**: The median response time - 50% of cases complete faster than this.

**Unit**: Milliseconds (ms)

**Interpretation**: Represents typical performance.

#### P95 Latency

**Definition**: 95th percentile response time - 95% of cases complete faster than this.

**Unit**: Milliseconds (ms)

**Threshold**: ≤ 3000ms (3 seconds)

**Interpretation**: Represents worst-case performance for most users. Critical for user experience.

**Why It Matters**: P95 is more important than average because it captures the experience of the slowest 5% of requests, which users notice most.

#### P99 Latency

**Definition**: 99th percentile response time - 99% of cases complete faster than this.

**Unit**: Milliseconds (ms)

**Interpretation**: Captures extreme outliers.

#### Mean Latency

**Definition**: Average response time across all cases.

**Unit**: Milliseconds (ms)

**Interpretation**: Overall average performance.

**Example**:
```
Latencies: [800ms, 900ms, 1000ms, 1100ms, 1200ms, 1500ms, 2000ms, 2500ms, 3000ms, 5000ms]

P50: 1200ms (median)
P95: 4600ms (95th percentile)
P99: 4960ms (99th percentile)
Mean: 1900ms (average)
```

### Total Tokens

**Definition**: Total number of tokens (input + output) used across all LLM calls.

**Why It Matters**: Helps understand resource usage and optimize prompts.

---

## Threshold Configuration

Thresholds determine pass/fail status for evaluations. Configure in your YAML file:

```yaml
# Minimum acceptable values (higher is better)
min_accuracy: 0.75          # 75% clinical accuracy
min_faithfulness: 0.80      # 80% faithfulness score
min_safety_score: 4.0       # 4.0/5.0 safety score

# Maximum acceptable values (lower is better)
max_cost_per_query: 0.10    # $0.10 per query
max_p95_latency: 3000.0     # 3000ms (3 seconds)
```

### Threshold Checking

All thresholds must be met for evaluation to pass:

```python
thresholds_met = {
    "accuracy": clinical_accuracy >= min_accuracy,
    "faithfulness": faithfulness >= min_faithfulness,
    "safety": avg_safety_score >= min_safety_score,
    "cost": cost_per_query <= max_cost_per_query,
    "latency": p95_latency <= max_p95_latency
}

all_thresholds_met = all(thresholds_met.values())
```

### Adjusting Thresholds

**For Development/Testing**:
```yaml
min_accuracy: 0.60          # Lower bar for experimentation
min_safety_score: 3.5
max_cost_per_query: 0.20    # Allow higher costs
```

**For Production**:
```yaml
min_accuracy: 0.85          # Higher bar for deployment
min_safety_score: 4.5
max_cost_per_query: 0.05    # Strict cost control
```

---

## Metric Relationships

### Accuracy vs. Safety

- High accuracy doesn't guarantee safety
- A model might correctly identify common conditions but miss dangerous ones
- Both metrics are essential

### Cost vs. Quality

- More expensive models (GPT-4o) often have higher accuracy
- Cheaper models (GPT-3.5) may be sufficient for some use cases
- Use A/B testing to find the optimal cost/quality tradeoff

### Latency vs. Accuracy

- Faster models may sacrifice some accuracy
- For real-time clinical use, latency is critical
- Batch processing can tolerate higher latency

---

## Best Practices

1. **Monitor Trends**: Track metrics over time to detect degradation
2. **Set Realistic Thresholds**: Based on your specific use case and requirements
3. **Use A/B Testing**: Compare models systematically before deployment
4. **Review Failures**: Manually review cases that fail safety checks
5. **Balance Metrics**: Don't optimize for one metric at the expense of others
6. **Consider Context**: Different specialties may have different threshold requirements

---

## Metric Limitations

### Clinical Accuracy
- Only measures top-3 accuracy, not ranking quality
- Doesn't capture partial credit for related diagnoses
- Depends on quality of expert labels

### Ragas Metrics
- Require API calls (additional cost)
- May fail if API is unavailable
- Automated metrics can't fully capture clinical nuance

### Judge Metrics
- Subjective - different judge models may score differently
- Expensive (requires additional LLM calls)
- May have biases based on judge model training

### Performance Metrics
- Latency varies with network conditions
- Cost depends on current API pricing
- Token usage varies with prompt engineering

---

## Further Reading

- [Ragas Documentation](https://docs.ragas.io/)
- [LangSmith Evaluation Guide](https://docs.smith.langchain.com/)
- Clinical evaluation best practices (see project references)
